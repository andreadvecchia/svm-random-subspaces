{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7812bcf4",
   "metadata": {},
   "source": [
    "\n",
    "# Random-Subspace SVM — Demo Experiment\n",
    "\n",
    "This notebook gives a **5‑minute, reproducible** tour of the core idea from the paper *Regularized ERM on Random Subspaces* (AISTATS 2021).\n",
    "It builds a small **Random‑Subspace SVM** (an ensemble of SVMs trained on random feature subsets) and compares it to a standard SVM on a synthetic dataset.\n",
    "\n",
    "**What you'll see**\n",
    "- quick install and run\n",
    "- simple, well‑commented reference implementation (scikit‑learn compatible)\n",
    "- sanity‑check metrics and a clean plot\n",
    "\n",
    "> Tip: Run `Runtime → Run all` to reproduce the results end‑to‑end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfb69c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Minimal dependencies for this demo:\n",
    "# (Install once per environment. Comment out if already installed.)\n",
    "# %pip install -q numpy scipy scikit-learn matplotlib\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Tuple\n",
    "from time import perf_counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import check_random_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059a83ac",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Create a toy high‑dimensional dataset\n",
    "We simulate a binary classification task with **50 features** where only a subset is informative.  \n",
    "This is a common regime where random subspaces can help.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f964aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RANDOM_SEED = 7\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=2000,\n",
    "    n_features=50,\n",
    "    n_informative=10,\n",
    "    n_redundant=10,\n",
    "    n_repeated=0,\n",
    "    n_classes=2,\n",
    "    n_clusters_per_class=2,\n",
    "    class_sep=1.2,\n",
    "    flip_y=0.02,\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21d55e7",
   "metadata": {},
   "source": [
    "\n",
    "## 2) A tiny Random‑Subspace SVM (sklearn‑compatible)\n",
    "\n",
    "We train **M base SVMs**, each on a **subset of features** (size `k`).  \n",
    "At inference, we **average decision functions** (or majority vote) for a stable, regularized predictor.\n",
    "\n",
    "This keeps the demo lightweight while conveying the core idea.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bd4d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RandomSubspaceSVM(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Ensemble of SVMs trained on random feature subsets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    base_estimator : sklearn classifier\n",
    "        Typically `LinearSVC()` or `SVC(kernel=\"rbf\")`.\n",
    "    M : int\n",
    "        Number of base models (subspaces).\n",
    "    k : int\n",
    "        Subspace dimension (number of features per model).\n",
    "    voting : {'hard','soft'}\n",
    "        'hard' uses majority vote; 'soft' averages decision_function.\n",
    "    random_state : int or numpy RandomState\n",
    "        Reproducibility.\n",
    "    scale : bool\n",
    "        If True, includes a StandardScaler before the SVM in each base learner.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_estimator=None, M=25, k=20, voting='soft', random_state=None, scale=True):\n",
    "        self.base_estimator = base_estimator if base_estimator is not None else LinearSVC(dual=False)\n",
    "        self.M = int(M)\n",
    "        self.k = int(k)\n",
    "        self.voting = voting\n",
    "        self.random_state = random_state\n",
    "        self.scale = scale\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        rs = check_random_state(self.random_state)\n",
    "        n_features = X.shape[1]\n",
    "        if self.k <= 0 or self.k > n_features:\n",
    "            raise ValueError(f\"k must be in [1, {n_features}] (got {self.k}).\")\n",
    "        \n",
    "        self.subspaces_: List[np.ndarray] = []\n",
    "        self.models_: List[Pipeline] = []\n",
    "        \n",
    "        for m in range(self.M):\n",
    "            feat_idx = rs.choice(n_features, size=self.k, replace=False)\n",
    "            self.subspaces_.append(np.sort(feat_idx))\n",
    "            \n",
    "            est = clone(self.base_estimator)\n",
    "            steps = []\n",
    "            if self.scale:\n",
    "                steps.append(('scaler', StandardScaler(with_mean=True)))\n",
    "            steps.append(('clf', est))\n",
    "            pipe = Pipeline(steps)\n",
    "            pipe.fit(X[:, feat_idx], y)\n",
    "            self.models_.append(pipe)\n",
    "        return self\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        if not hasattr(self, 'models_'):\n",
    "            raise RuntimeError(\"Model not fitted.\")\n",
    "        # Average decision function (soft voting)\n",
    "        scores = None\n",
    "        for pipe, idx in zip(self.models_, self.subspaces_):\n",
    "            # Ensure the estimator has decision_function; fall back to predict_proba if available.\n",
    "            clf = pipe.named_steps['clf']\n",
    "            if hasattr(clf, 'decision_function'):\n",
    "                s = pipe.decision_function(X[:, idx])\n",
    "            elif hasattr(clf, 'predict_proba'):\n",
    "                proba = pipe.predict_proba(X[:, idx])\n",
    "                # Map proba to signed score: P(y=1)-P(y=0)\n",
    "                s = proba[:,1] - proba[:,0]\n",
    "            else:\n",
    "                # Last resort: use predicted labels as +/-1\n",
    "                preds = pipe.predict(X[:, idx])\n",
    "                s = (preds * 2 - 1).astype(float)\n",
    "            scores = s if scores is None else scores + s\n",
    "        return scores / self.M\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.voting == 'soft':\n",
    "            return (self.decision_function(X) >= 0).astype(int)\n",
    "        else:\n",
    "            # Hard voting\n",
    "            votes = None\n",
    "            for pipe, idx in zip(self.models_, self.subspaces_):\n",
    "                pred = pipe.predict(X[:, idx])\n",
    "                votes = pred if votes is None else votes + pred\n",
    "            # majority threshold\n",
    "            return (votes >= (self.M/2)).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5963c7d",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Train baseline SVM vs. Random‑Subspace SVM\n",
    "We compare a strong baseline SVM with RBF kernel to an ensemble of `M=25` linear SVMs on `k=20`‑dimensional subspaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4630681",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Baseline\n",
    "baseline = Pipeline([\n",
    "    ('scaler', StandardScaler(with_mean=True)),\n",
    "    ('clf', SVC(kernel='rbf', C=1.0, gamma='scale', probability=False))\n",
    "])\n",
    "\n",
    "# Random‑Subspace ensemble\n",
    "rssvm = RandomSubspaceSVM(\n",
    "    base_estimator=LinearSVC(dual=False), \n",
    "    M=25, k=20, voting='soft', random_state=RANDOM_SEED, scale=True\n",
    ")\n",
    "\n",
    "t0 = perf_counter(); baseline.fit(X_train, y_train); t_baseline = perf_counter()-t0\n",
    "t0 = perf_counter(); rssvm.fit(X_train, y_train);     t_rssvm   = perf_counter()-t0\n",
    "\n",
    "yhat_base = baseline.predict(X_test)\n",
    "yhat_rss  = rssvm.predict(X_test)\n",
    "\n",
    "acc_base = accuracy_score(y_test, yhat_base)\n",
    "acc_rss  = accuracy_score(y_test, yhat_rss)\n",
    "\n",
    "print(f\"Baseline SVM (RBF):  acc={acc_base:.3f}  train_time={t_baseline*1e3:.0f} ms\")\n",
    "print(f\"Random‑Subspace SVM: acc={acc_rss:.3f}  train_time={t_rssvm*1e3:.0f} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74175a1f",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Quick cross‑validation (sanity check)\n",
    "We do a 3‑fold CV on the training set. (Fast and indicative; tune as needed.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e845689",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scores_base = cross_val_score(baseline, X_train, y_train, cv=3, n_jobs=None)\n",
    "scores_rss  = cross_val_score(\n",
    "    RandomSubspaceSVM(base_estimator=LinearSVC(dual=False), M=15, k=15, voting='soft', random_state=RANDOM_SEED),\n",
    "    X_train, y_train, cv=3, n_jobs=None\n",
    ")\n",
    "print(f\"CV accuracy — Baseline SVM:  mean={scores_base.mean():.3f} ± {scores_base.std():.3f}\")\n",
    "print(f\"CV accuracy — RSSVM (M=15,k=15): mean={scores_rss.mean():.3f} ± {scores_rss.std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e29a989",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Visualize performance\n",
    "A simple confusion matrix for each model on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e73377",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "disp1 = ConfusionMatrixDisplay.from_predictions(y_test, yhat_base, ax=axes[0], colorbar=False)\n",
    "axes[0].set_title(\"Baseline SVM (RBF)\")\n",
    "\n",
    "disp2 = ConfusionMatrixDisplay.from_predictions(y_test, yhat_rss, ax=axes[1], colorbar=False)\n",
    "axes[1].set_title(\"Random‑Subspace SVM\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbc79a4",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Next steps\n",
    "- Replace this lightweight reference with the **full experimental pipeline** in `scripts/` when ready.\n",
    "- Try other subspace sizes `k`, number of models `M`, or an RBF base SVM.\n",
    "- Port this class to the library (e.g., `src/`) and add unit tests.\n",
    "- Consider Nyström kernel approximations for large‑scale experiments.\n",
    "\n",
    "**Reproducibility:** set `RANDOM_SEED` and keep `train/test` splits fixed when comparing settings.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
